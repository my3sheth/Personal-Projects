# -*- coding: utf-8 -*-
"""Mini Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kxR5U2BeYfDzhYca5gF0zTmB04FPwcrd

#### **INTRODUCTION**
"""

import kagglehub
import os
import warnings
import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectFromModel
from imblearn.over_sampling import BorderlineSMOTE
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, classification_report,
    roc_curve, auc, precision_recall_curve
)

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning, module='sklearn')

# Dataset Download and File Verification
path = kagglehub.dataset_download("dhoogla/csecicids2018")
print("Path to dataset files:", path)

# Extract the required Parquet file and convert it to CSV
parquet_file = 'Botnet-Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet'
csv_file = 'Botnet-Friday-02-03-2018_TrafficForML_CICFlowMeter.csv'

parquet_path = os.path.join(path, parquet_file)
csv_path = os.path.join(path, csv_file)

# Convert Parquet to CSV
try:
    df = pd.read_parquet(parquet_path)
    df.to_csv(csv_path, index=False)
    print(f"Conversion successful.")
except Exception as e:
    print(f"Error converting Parquet to CSV: {e}")
    raise e

# Load the newly converted CSV file
try:
    data = pd.read_csv(csv_path)
    print("Dataset loaded successfully.")
except FileNotFoundError as e:
    print("File not found. Please check the dataset path and file name.")
    raise e

data.head()

print("Shape of raw data:", data.shape)

"""#### **MODULE 1: Data Preprocessing**"""

print(data.info())

print(data.describe())

data = data.dropna().drop_duplicates()
if 'Label' not in data.columns:
    raise ValueError("Target column 'Label' is missing.")

print("Total missing values in dataset:", data.isnull().sum().sum())

print("Shape of cleaned data:", data.shape)

num_classes = data['Label'].nunique()
print(f"Number of unique classes: {num_classes}")
print("Multiclass classification" if num_classes > 2 else "Binary classification")

"""#### **MODULE 2: Feature Engineering and Selection**"""

label_encoder = LabelEncoder()
data['Label'] = label_encoder.fit_transform(data['Label'])

scaler = StandardScaler()
scaled_features = pd.DataFrame(
    scaler.fit_transform(data.iloc[:, :-1]),
    columns=data.columns[:-1]
).astype('float64')

data = pd.concat([scaled_features, data.iloc[:, -1].reset_index(drop=True)], axis=1)

data.head()

# Define features and target variable
X = data.drop(columns=['Label'])
y = data['Label']

# Remove Highly Correlated Features
correlation_matrix = X.corr().abs()
upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]
X_filtered = X.drop(columns=to_drop)
print(f"Removed {len(to_drop)} highly correlated features.")

# Feature Scaling
X_scaled = pd.DataFrame(scaler.fit_transform(X_filtered), columns=X_filtered.columns).astype('float64')

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Check class distribution before resampling
print("Class distribution before SMOTE:", Counter(y_train))

# Apply Borderline SMOTE
smote = BorderlineSMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Check class distribution after resampling
print("Class distribution after SMOTE:", Counter(y_train_resampled))

num_features = min(15, X_train.shape[1])
selector_logistic = SelectKBest(score_func=mutual_info_classif, k=num_features)
X_train_log_selected = selector_logistic.fit_transform(X_train_resampled, y_train_resampled)
X_test_log_selected = selector_logistic.transform(X_test)

print(f"Selected {num_features} best features.")

"""#### **MODULE 3: ML Algorithms**"""

def train_evaluate_model(model, name, X_train, X_test):
    """Train model and evaluate its accuracy and F1-score."""
    model.fit(X_train, y_train_resampled)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    return acc, f1

models = {
    "GaussianNB": GaussianNB(),
    "Logistic Regression": LogisticRegression(),
    "Linear Discriminant Analysis": LinearDiscriminantAnalysis()
}

# Train models on the selected features
results = {}

for name, model in models.items():
    X_train_selected = X_train_log_selected
    X_test_selected = X_test_log_selected

    acc, f1 = train_evaluate_model(model, name, X_train_selected, X_test_selected)
    results[name] = {"accuracy": acc, "f1": f1}

y_pred_gnb = pd.Series(models["GaussianNB"].predict(X_test_log_selected))
y_pred_lr = pd.Series(models["Logistic Regression"].predict(X_test_log_selected))
y_pred_lda = pd.Series(models["Linear Discriminant Analysis"].predict(X_test_log_selected))

# Ensure results dictionary contains valid data
f1_scores = []
valid_models = []

for name in models.keys():
    if name in results:
        # Check if results[name] is a tuple or list with at least 2 elements
        if isinstance(results[name], (list, tuple)) and len(results[name]) > 1:
            f1_scores.append(results[name][1])  # Extract F1-score
            valid_models.append(name)
        elif isinstance(results[name], dict) and "f1" in results[name]:
            f1_scores.append(results[name]["f1"])  # Extract if stored as dictionary
            valid_models.append(name)
        else:
            print(f"Skipping {name}: Invalid format or missing F1-score.")
    else:
        print(f"Skipping {name}: Model not found in results.")

# Convert to NumPy array if valid F1 scores exist
if f1_scores:
    f1_scores = np.array(f1_scores)
    weights = f1_scores / f1_scores.sum()
    print("Weights calculated successfully:", dict(zip(valid_models, weights)))
else:
    print("Error: No valid F1 scores found!")

"""#### **MODULE 4: Meta-Ensemble Learning**"""

# Define the Voting Classifier (Soft Voting for probability-based aggregation)
ensemble = VotingClassifier(
    estimators=[(name, model) for name, model in models.items()],
    voting='soft',
    weights=weights
)

ensemble.fit(X_train_resampled, y_train_resampled)

# Evaluate ensemble performance
y_pred_ensemble = ensemble.predict(X_test)
ensemble_acc = accuracy_score(y_test, y_pred_ensemble)
ensemble_f1 = f1_score(y_test, y_pred_ensemble, average='weighted')

df_predictions = pd.DataFrame({
    "Actual Label": y_test.values,
    "GNB Prediction": y_pred_gnb,
    "LR Prediction": y_pred_lr,
    "LDA Prediction": y_pred_lda,
    "Ensemble Prediction": y_pred_ensemble
})

# Print first few rows of predictions
print(df_predictions.tail())

# Print value counts for each model's predictions
print("\n--- Prediction Value Counts ---")
for col in df_predictions.columns[1:]:  # Exclude "Actual Label"
    print(f"\n{col} Value Counts:")
    print(df_predictions[col].value_counts())

# Print individual model performances
print("\n--- Individual Model Performances ---")
for name, metrics in results.items():
    if isinstance(metrics, dict) and 'accuracy' in metrics and 'f1' in metrics:
        acc, f1 = metrics['accuracy'], metrics['f1']
        print(f"{name} - Accuracy: {acc:.4f}, F1 Score: {f1:.4f}")
    else:
        print(f"Skipping {name}: Invalid format or missing data ({metrics})")

# Compute and print ensemble model performance
ensemble_acc, ensemble_f1 = train_evaluate_model(ensemble, "Ensemble Model", X_train_resampled, X_test)

print("\n--- Ensemble Model Performance ---")
print(f"Ensemble Model - Accuracy: {ensemble_acc:.4f}, F1 Score: {ensemble_f1:.4f}")

# Ensure the ensemble performs better
assert all(ensemble_acc > metrics['accuracy'] for metrics in results.values() if isinstance(metrics, dict)), "Ensemble accuracy is not higher!"

# Evaluate performance using cross-validation
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(ensemble, X_train_resampled, y_train_resampled, cv=kfold, scoring='accuracy')
print(f'Mean CV Accuracy: {np.mean(cv_scores):.4f}')

"""#### **MODULE 5: Evaluation and Visualization**"""

fpr, tpr, _ = roc_curve(y_test, ensemble.predict_proba(X_test)[:, 1])
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC-AUC Curve')
plt.legend()
plt.show()

precision, recall, _ = precision_recall_curve(y_test, ensemble.predict_proba(X_test)[:, 1])
pr_auc = auc(recall, precision)
plt.plot(recall, precision, marker='.', label=f'PR AUC = {pr_auc:.4f}')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()
plt.show()

conf_matrix = confusion_matrix(y_test, y_pred_ensemble)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - Ensemble Model")
plt.show()

# Save Attack Distribution
attack_dist = pd.DataFrame({'Label': y_test}).value_counts().reset_index()
attack_dist.columns = ['Label', 'Count']
attack_dist.to_csv("Attack_Distribution.csv", index=False)

# Save Classification Report
classification_rep = pd.DataFrame(classification_report(y_test, y_pred_ensemble, output_dict=True)).transpose()
classification_rep.reset_index(inplace=True)
classification_rep.to_csv("Classification_Report.csv", index=False)

# Save Model Performance Metrics
model_accuracies = pd.DataFrame({
    'Model': list(results.keys()) + ['Ensemble Model'],
    'Accuracy': [results[name]["accuracy"] for name in models.keys()] + [ensemble_acc],
    'F1 Score': [results[name]["f1"] for name in models.keys()] + [ensemble_f1]
})

model_accuracies.to_csv("Model_Accuracies.csv", index=False)

# Save Feature Importance Report
logistic_model = models["Logistic Regression"]
selected_features = X_train.columns[selector_logistic.get_support()]
feature_importance = pd.DataFrame({
    'Feature': selected_features,
    'Importance': abs(logistic_model.coef_[0])
})
feature_importance = feature_importance.sort_values(by="Importance", ascending=False)
feature_importance.to_csv("Feature_Importance.csv", index=False)

print("CSV files generated successfully for Power BI visualization!")